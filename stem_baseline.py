# -*- coding: utf-8 -*-
"""stem_baseline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MWvaExiQtYZJjSCw60chKT3Xgjp4YjGs
"""

# from google.colab import drive
# drive.mount('/content/gdrive')

# import os
# os.chdir("/content/gdrive/MyDrive/PMF/pmf/datasets")

# !pip install arch

# !pip install pm4py

# !python -c "import pm4py"

# !pip install pm4py.objects.log.log

import numpy as np
import pm4py
import pytz
from datetime import datetime

from pm4py.algo.discovery.dfg.variants import native as dfg_factory
from collections import Counter
from pm4py.algo.filtering.log.timestamp import timestamp_filter
from pm4py.algo.discovery.dfg.variants import native

from pm4py.objects.dfg.utils.dfg_utils import infer_start_activities, infer_end_activities
from pm4py.objects.log.obj import EventLog, Trace, Event
from pm4py.objects.log.exporter.xes.variants import etree_xes_exp as exporter

import os
os.environ["CUDA_VISIBLE_DEVICES"] = "1"

class ActivityPair:

    def __init__(self, a1, a2, timestamp, event, event2, trace_no):
        self.a1 = a1
        self.a2 = a2
        self.timestamp = timestamp
        self.event = event
        self.event2 = event2
        self.trace_no = trace_no

    def __str__(self):
        return self.a1 + ' before ' + self.a2 + ' at ' + str(self.timestamp)

    def __gt__(self, other):
        if self.timestamp > other.timestamp:
            return True
        else:
            return False


def read_data_equisize(no_intervals, interval_width, sorted_aps, act_map, log, dataset):
    no_act = len(act_map.keys())

    dfg_time_matrix = np.zeros([no_intervals, no_act, no_act], dtype=int)

    interval_timing = []

    no_events_sums = 0
    no_events_logs = 0
    no_dfs = 0

    shadow_dict = {}
    shadow_event_l = 0
    logs_finished = 0
    log_progression = {}
    for i in range(0, no_intervals):
        print('Interval ', i+1, '/', no_intervals)
        lower_bound = i * interval_width
        upper_bound = (i+1) * interval_width
        if i == (no_intervals - 1):
            upper_bound = len(sorted_aps)

        dfs = sorted_aps[lower_bound:upper_bound]
        no_dfs += len(dfs)

        print('#DFS:', len(dfs))

        empty_mat = np.zeros([no_act, no_act], dtype=float)

        # For output
        filtered_events = {}
        start = Event()
        end = Event()
        start['concept:name'] = str(act_map['start'])
        end['concept:name'] = str(act_map['end'])
        highest = datetime(1970, 1, 1, tzinfo=pytz.UTC)
        lowest = datetime(2050, 1, 1, tzinfo=pytz.UTC)

        log_dfs = {}
        for df in dfs:
            if df.trace_no not in log_dfs.keys():
                log_dfs[df.trace_no] = []
            log_dfs[df.trace_no].append(df)

        for trace_no, dfss in log_dfs.items():
            # print('\nTrace:', trace_no)
            sorted_dfs = sorted(dfss)
            filtered_events[trace_no] = []
            for df in sorted_dfs:
                # print(df)
                filtered_events[trace_no].append(df.event)
                no_events_sums += 1
            filtered_events[trace_no].append(sorted_dfs[len(sorted_dfs) - 1].event2)

            no_events_sums += 1

        for trace_no, events in filtered_events.items():
            empty_mat[act_map['start'], act_map[events[0]['concept:name']]] += 1
            empty_mat[act_map[events[-1]['concept:name']], act_map['end']] += 1

        # Export filtered events to interval event logs
        new_log = EventLog()
        no_eve = 0
        for t, trace in enumerate(log):
            if i < 50:
                if t not in shadow_dict.keys():
                    shadow_dict[t] = []
            new_trace = Trace()
            for trace_no, events in filtered_events.items():
                if t == trace_no:
                    for event in trace:
                        if event in events:
                            if event['time:timestamp'] < lowest:
                                lowest = event['time:timestamp']
                            if event['time:timestamp'] > highest:
                                highest = event['time:timestamp']
                            new_event = Event()
                            new_event['concept:name'] = str(act_map[event['concept:name']])
                            new_trace.append(new_event)
                            no_events_logs += 1
                            no_eve += 1
                            if i < 50:
                                shadow_dict[t].append(new_event)
                                shadow_event_l += 1
            if len(new_trace) > 0:
                # new_trace.append(end)
                new_log.append(new_trace)

        # exporter.apply(new_log, './logs/' + dataset + '_log_interval_' + str(i) + '-'
        #                + str(no_intervals) + '_equisize.xes')
        exporter.apply(new_log, '/mnt/sdb/yujia/pmf/logs/' + dataset + '_log_interval_' + str(i) + '-'
                       + str(no_intervals) + '_equisize.xes')

# /mnt/sdb/yujia/pmf/logs/InternationalDeclarations

        print('#Events:', no_eve)
        print('#Events log:', no_events_logs)
        print('#Events shadow low:', shadow_event_l)
        for act_pair in dfs:
            a1 = act_map[act_pair.a1]
            a2 = act_map[act_pair.a2]
            empty_mat[a1, a2] += 1

        dfg_time_matrix[i] = empty_mat

        interval_timing.append((lowest, highest))

    print('Event sums:', no_events_sums)
    print('Event logs:', no_events_logs)
    print('#DFS:', no_dfs)
    print('Logs finished:', logs_finished)
    return dfg_time_matrix, interval_timing


def read_data_equitemp(no_intervals, interval_width, sorted_aps, act_map, log, dataset):
    timestamps = pm4py.get_attribute_values(log, 'time:timestamp')
    print('Earliest:', min(timestamps))
    print('Latest:', max(timestamps))
    interval_length = (max(timestamps) - min(timestamps)) / no_intervals
    print('Interval length:', interval_length)

    no_act = len(act_map.keys())

    dfg_time_matrix = np.zeros([no_intervals, no_act, no_act], dtype=int)

    interval_timing = []
    no_events_sums = 0
    no_events_logs = 0
    no_dfs = 0
    for i in range(0, no_intervals):
        print('Interval ', i, '/', no_intervals)
        lower_bound = min(timestamps) + i * interval_length
        if i == (no_intervals - 1):
            upper_bound = min(timestamps) + (i + 1) * interval_length * 2
        else:
            upper_bound = min(timestamps) + (i + 1) * interval_length
        lb = lower_bound
        ub = upper_bound
        print(lb)
        print(ub)

        dfs = []
        empty_mat = np.zeros([no_act, no_act], dtype=float)

        filtered_events = {}
        start = Event()
        end = Event()
        start['concept:name'] = str(act_map['start'])
        end['concept:name'] = str(act_map['end'])

        print('start: ', start, '\n end: ', end)

        highest = datetime(1970, 1, 1, tzinfo=pytz.UTC)
        lowest = datetime(2050, 1, 1, tzinfo=pytz.UTC)

        count = 0
        for df in sorted_aps:
            if ub > df.event2['time:timestamp'] >= lb:# and ub > df.event['time:timestamp'] >= lb:
                dfs.append(df)

        no_dfs += len(dfs)

        log_dfs = {}
        for df in dfs:
            if df.trace_no not in log_dfs.keys():
                log_dfs[df.trace_no] = []
            log_dfs[df.trace_no].append(df)

        for trace_no, dfss in log_dfs.items():
            # print('\nTrace:', trace_no)
            sorted_dfs = sorted(dfss)
            filtered_events[trace_no] = []
            for df in sorted_dfs:
                # print(df)
                filtered_events[trace_no].append(df.event)
                no_events_sums += 1
            filtered_events[trace_no].append(sorted_dfs[len(sorted_dfs)-1].event2)
            no_events_sums += 1

        print('#traces:', len(log_dfs))

        for trace_no, events in filtered_events.items():
            empty_mat[act_map['start'], act_map[events[0]['concept:name']]] += 1
            empty_mat[act_map[events[-1]['concept:name']], act_map['end']] += 1

        # Export filtered events to interval event logs
        new_log = EventLog()
        no_eve = 0
        for t, trace in enumerate(log):
            new_trace = Trace()
            # new_trace.append(start)
            for trace_no, events in filtered_events.items():
                if t == trace_no:
                    for event in trace:
                        if event in events:
                            if event['time:timestamp'] < lowest:
                                lowest = event['time:timestamp']
                            if event['time:timestamp'] > highest:
                                highest = event['time:timestamp']
                            new_event = Event()
                            new_event['concept:name'] = str(act_map[event['concept:name']])
                            new_trace.append(new_event)
                            no_events_sums += 1
                            no_eve += 1
            if len(new_trace) > 0:
                # new_trace.append(end)
                new_log.append(new_trace)
        # exporter.apply(new_log, './logs/' + dataset + '_log_interval_' + str(i) + '-'
        #                + str(no_intervals) + '_equitemp.xes')
        exporter.apply(new_log, '/mnt/sdb/yujia/pmf/logs/' + dataset + '_log_interval_' + str(i) + '-'
                       + str(no_intervals) + '_equitemp.xes')

        # print('no eve:', no_eve)
        for act_pair in dfs:
            a1 = act_map[act_pair.a1]
            a2 = act_map[act_pair.a2]
            empty_mat[a1, a2] += 1

        dfg_time_matrix[i] = empty_mat
        interval_timing.append((lowest, highest))
    print('Event sums:', no_events_sums)
    print('Event logs:', no_events_logs)
    print('#DFS:', no_dfs)

    return dfg_time_matrix, interval_timing


def determine_cutoff_point(act_map, dfg_time_matrix, no_pairs):
    counts = []
    for act, a in act_map.items():
        for act2, a2 in act_map.items():
            counts.append(np.sum(dfg_time_matrix[:, a, a2]))
    fc = Counter(counts)
    no_found = 0
    cutoff = 0
    for sum in sorted(fc.keys(), reverse=True):
        cutoff = sum
        no_found += fc[sum]
        if no_found > no_pairs:
            break
    print('CUTOFF:', cutoff, '#pairs:', no_found)

    return cutoff

import numpy as np

from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.exponential_smoothing import ExponentialSmoothing
from statsmodels.tsa.ar_model import AutoReg as AR
from arch.univariate import arch_model as ARCH
from statsmodels.tsa.holtwinters import SimpleExpSmoothing as SES, ExponentialSmoothing as HW
from sklearn.preprocessing import MinMaxScaler


class Scaler:


    def __init__(self, low, up, array_in):
        X = array_in.copy()

        self.x_min = X.min(axis=0)
        self.x_max = X.max(axis=0)
        self.low = low
        self.up = up


    def scale(self, X_in):
        X = X_in.copy()
        X_std = (X - self.x_min) / (self.x_max - self.x_min)
        X_scaled = X_std * (self.up - self.low) + self.low
        return X_scaled

    def reverse(self, X_in):
        X = X_in.copy()
        X = X + self.low
        X = X * (self.up - self.low)
        X_ret =  (self.x_max - self.x_min) / (X - self.x_min)

        return X_ret


class NAVf:

    class Fitted:

        def __init__(self, x):
            self.x = x

        def forecast(self, horizon):
            mean = np.mean(self.x)
            # print('Naive:', np.full(shape=(1, horizon), fill_value=mean))
            return np.full(shape=(1, horizon), fill_value=mean)

    def __init__(self):
        pass

    def fit(self, x, horizon):
        return self.Fitted(x).forecast(horizon)[0]


class ARf:

    def __init__(self, d):
        self.d = d

    def fit(self, x, horizon):
        fit = AR(x, lags=self.d, old_names=False).fit()
        point_forecast = fit.get_prediction(len(x), len(x)+horizon-1)

        # print(f'AR {self.d}:', point_forecast.predicted_mean)
        # print(point_forecast.conf_int(0.05))

        return point_forecast.predicted_mean


class ARIMAf:

    def __init__(self, p, d, q):
        self.p = p
        self.d = d
        self.q = q

    def fit(self, x, horizon):
        fit = ARIMA(x, order=(self.p, self.d, self.q)).fit()
        point_forecast = fit.get_forecast(horizon)
        conf_int = fit.conf_int()

        # print('ARIMA:', point_forecast.predicted_mean)
        # print(point_forecast.conf_int())
        return point_forecast.predicted_mean


class ESf:

    def __init__(self):
        pass

    def fit(self, x, horizon):
        fit = ExponentialSmoothing(x).fit()
        point_forecast = fit.get_prediction(len(x), len(x)+horizon-1)
        print('ETS', point_forecast.predicted_mean)
        # print('ETS', point_forecast.conf_int())

        return point_forecast.predicted_mean


class SESf:

    def __init__(self):
        pass

    def fit(self, x, horizon):
        fit = SES(x, initialization_method='estimated').fit()
        # print('SES', fit.forecast(horizon))

        return fit.forecast(horizon)


class HWf:

    def __init__(self):
        pass

    def fit(self, x, horizon):
        fit = HW(x, initialization_method='estimated').fit()

        # print('HW', fit.forecast(horizon))
        return fit.forecast(horizon)


class GARCHf:

    def __init__(self):
        pass

    def fit(self, x, horizon):
        garch = ARCH(x)
        model = garch.fit(disp='off')
        y_hat = model.forecast(horizon=horizon)

        pred = y_hat.mean.iloc[[-1]].values[0]
        var = np.sqrt(y_hat.variance.iloc[[-1]].values) * 1.96
        conf_low = pred + var[0]
        conf_high = pred - var[0]
        return pred





import numpy as np
import pm4py
import pytz
from datetime import datetime

from pm4py.algo.discovery.dfg.variants import native as dfg_factory
from collections import Counter
from pm4py.algo.filtering.log.timestamp import timestamp_filter
from pm4py.algo.discovery.dfg.variants import native

from pm4py.objects.dfg.utils.dfg_utils import infer_start_activities, infer_end_activities
# from pm4py.objects.log.log import EventLog, Trace, Event
from pm4py.objects.log.obj import EventLog, Trace, Event
from pm4py.objects.log.exporter.xes.variants import etree_xes_exp as exporter

class ActivityPair:

    def __init__(self, a1, a2, timestamp, event, event2, trace_no):
        self.a1 = a1
        self.a2 = a2
        self.timestamp = timestamp
        self.event = event
        self.event2 = event2
        self.trace_no = trace_no

    def __str__(self):
        return self.a1 + ' before ' + self.a2 + ' at ' + str(self.timestamp)

    def __gt__(self, other):
        if self.timestamp > other.timestamp:
            return True
        else:
            return False


def read_data_equisize(no_intervals, interval_width, sorted_aps, act_map, log, dataset):
    no_act = len(act_map.keys())

    dfg_time_matrix = np.zeros([no_intervals, no_act, no_act], dtype=int)

    interval_timing = []

    no_events_sums = 0
    no_events_logs = 0
    no_dfs = 0

    shadow_dict = {}
    shadow_event_l = 0
    logs_finished = 0
    log_progression = {}
    for i in range(0, no_intervals):
        print('Interval ', i+1, '/', no_intervals)
        lower_bound = i * interval_width
        upper_bound = (i+1) * interval_width
        if i == (no_intervals - 1):
            upper_bound = len(sorted_aps)

        dfs = sorted_aps[lower_bound:upper_bound]
        no_dfs += len(dfs)

        print('#DFS:', len(dfs))

        empty_mat = np.zeros([no_act, no_act], dtype=float)

        # For output
        filtered_events = {}
        start = Event()
        end = Event()
        start['concept:name'] = str(act_map['start'])
        end['concept:name'] = str(act_map['end'])
        highest = datetime(1970, 1, 1, tzinfo=pytz.UTC)
        lowest = datetime(2050, 1, 1, tzinfo=pytz.UTC)

        log_dfs = {}
        for df in dfs:
            if df.trace_no not in log_dfs.keys():
                log_dfs[df.trace_no] = []
            log_dfs[df.trace_no].append(df)

        for trace_no, dfss in log_dfs.items():
            # print('\nTrace:', trace_no)
            sorted_dfs = sorted(dfss)
            filtered_events[trace_no] = []
            for df in sorted_dfs:
                # print(df)
                filtered_events[trace_no].append(df.event)
                no_events_sums += 1
            filtered_events[trace_no].append(sorted_dfs[len(sorted_dfs) - 1].event2)

            no_events_sums += 1

        for trace_no, events in filtered_events.items():
            empty_mat[act_map['start'], act_map[events[0]['concept:name']]] += 1
            empty_mat[act_map[events[-1]['concept:name']], act_map['end']] += 1

        # Export filtered events to interval event logs
        new_log = EventLog()
        no_eve = 0
        for t, trace in enumerate(log):
            if i < 50:
                if t not in shadow_dict.keys():
                    shadow_dict[t] = []
            new_trace = Trace()
            for trace_no, events in filtered_events.items():
                if t == trace_no:
                    for event in trace:
                        if event in events:
                            if event['time:timestamp'] < lowest:
                                lowest = event['time:timestamp']
                            if event['time:timestamp'] > highest:
                                highest = event['time:timestamp']
                            new_event = Event()
                            new_event['concept:name'] = str(act_map[event['concept:name']])
                            new_trace.append(new_event)
                            no_events_logs += 1
                            no_eve += 1
                            if i < 50:
                                shadow_dict[t].append(new_event)
                                shadow_event_l += 1
            if len(new_trace) > 0:
                # new_trace.append(end)
                new_log.append(new_trace)

        # exporter.apply(new_log, './logs/' + dataset + '_log_interval_' + str(i) + '-'
        #                + str(no_intervals) + '_equisize.xes')
        exporter.apply(new_log, '/mnt/sdb/yujia/pmf/logs/' + dataset + '_log_interval_' + str(i) + '-'
                       + str(no_intervals) + '_equisize.xes')

        print('#Events:', no_eve)
        print('#Events log:', no_events_logs)
        print('#Events shadow low:', shadow_event_l)
        for act_pair in dfs:
            a1 = act_map[act_pair.a1]
            a2 = act_map[act_pair.a2]
            empty_mat[a1, a2] += 1

        dfg_time_matrix[i] = empty_mat

        interval_timing.append((lowest, highest))

    print('Event sums:', no_events_sums)
    print('Event logs:', no_events_logs)
    print('#DFS:', no_dfs)
    print('Logs finished:', logs_finished)
    return dfg_time_matrix, interval_timing


def read_data_equitemp(no_intervals, interval_width, sorted_aps, act_map, log, dataset):
    # timestamps = pm4py.get_attribute_values(log, 'time:timestamp')
    timestamps = pm4py.get_event_attribute_values(log, 'time:timestamp')

    # pm4py.statistics.attributes.log.get.get_attribute_values
    print('Earliest:', min(timestamps))
    print('Latest:', max(timestamps))
    interval_length = (max(timestamps) - min(timestamps)) / no_intervals
    print('Interval length:', interval_length)

    no_act = len(act_map.keys())

    dfg_time_matrix = np.zeros([no_intervals, no_act, no_act], dtype=int)

    interval_timing = []
    no_events_sums = 0
    no_events_logs = 0
    no_dfs = 0
    for i in range(0, no_intervals):
        print('Interval ', i, '/', no_intervals)
        lower_bound = min(timestamps) + i * interval_length
        if i == (no_intervals - 1):
            upper_bound = min(timestamps) + (i + 1) * interval_length * 2
        else:
            upper_bound = min(timestamps) + (i + 1) * interval_length
        lb = lower_bound
        ub = upper_bound
        print(lb)
        print(ub)

        dfs = []
        empty_mat = np.zeros([no_act, no_act], dtype=float)

        filtered_events = {}
        start = Event()
        end = Event()
        start['concept:name'] = str(act_map['start'])
        end['concept:name'] = str(act_map['end'])
        highest = datetime(1970, 1, 1, tzinfo=pytz.UTC)
        lowest = datetime(2050, 1, 1, tzinfo=pytz.UTC)

        count = 0
        for df in sorted_aps:
            if ub > df.event2['time:timestamp'] >= lb:# and ub > df.event['time:timestamp'] >= lb:
                dfs.append(df)

        no_dfs += len(dfs)

        log_dfs = {}
        for df in dfs:
            if df.trace_no not in log_dfs.keys():
                log_dfs[df.trace_no] = []
            log_dfs[df.trace_no].append(df)

        for trace_no, dfss in log_dfs.items():
            # print('\nTrace:', trace_no)
            sorted_dfs = sorted(dfss)
            filtered_events[trace_no] = []
            for df in sorted_dfs:
                # print(df)
                filtered_events[trace_no].append(df.event)
                no_events_sums += 1
            filtered_events[trace_no].append(sorted_dfs[len(sorted_dfs)-1].event2)
            no_events_sums += 1

        print('#traces:', len(log_dfs))

        for trace_no, events in filtered_events.items():
            empty_mat[act_map['start'], act_map[events[0]['concept:name']]] += 1
            empty_mat[act_map[events[-1]['concept:name']], act_map['end']] += 1

        # Export filtered events to interval event logs
        new_log = EventLog()
        no_eve = 0
        for t, trace in enumerate(log):
            new_trace = Trace()
            # new_trace.append(start)
            for trace_no, events in filtered_events.items():
                if t == trace_no:
                    for event in trace:
                        if event in events:
                            if event['time:timestamp'] < lowest:
                                lowest = event['time:timestamp']
                            if event['time:timestamp'] > highest:
                                highest = event['time:timestamp']
                            new_event = Event()
                            new_event['concept:name'] = str(act_map[event['concept:name']])
                            new_trace.append(new_event)
                            no_events_sums += 1
                            no_eve += 1
            if len(new_trace) > 0:
                # new_trace.append(end)
                new_log.append(new_trace)
        # exporter.apply(new_log, './logs/' + dataset + '_log_interval_' + str(i) + '-'
        #                + str(no_intervals) + '_equitemp.xes')
        exporter.apply(new_log, '/mnt/sdb/yujia/pmf/logs/' + dataset + '_log_interval_' + str(i) + '-'
                       + str(no_intervals) + '_equitemp.xes')

        # print('no eve:', no_eve)
        for act_pair in dfs:
            a1 = act_map[act_pair.a1]
            a2 = act_map[act_pair.a2]
            empty_mat[a1, a2] += 1

        dfg_time_matrix[i] = empty_mat
        interval_timing.append((lowest, highest))
    print('Event sums:', no_events_sums)
    print('Event logs:', no_events_logs)
    print('#DFS:', no_dfs)

    return dfg_time_matrix, interval_timing


def determine_cutoff_point(act_map, dfg_time_matrix, no_pairs):
    counts = []
    for act, a in act_map.items():
        for act2, a2 in act_map.items():
            counts.append(np.sum(dfg_time_matrix[:, a, a2]))
    fc = Counter(counts)
    no_found = 0
    cutoff = 0
    for sum in sorted(fc.keys(), reverse=True):
        cutoff = sum
        no_found += fc[sum]
        if no_found > no_pairs:
            break
    print('CUTOFF:', cutoff, '#pairs:', no_found)

    return cutoff

import pm4py
import json
import os
from pm4py.objects.log.importer.xes import importer as xes_importer
import numpy as np
from math import sqrt

from sklearn.metrics import mean_squared_error, precision_score, recall_score, mean_absolute_error
from scipy.spatial import distance

# from import_data import read_data_equisize, read_data_equitemp, determine_cutoff_point, ActivityPair
# from forecasting import ARf, ARIMAf, SESf, HWf, NAVf, ESf, GARCHf
# from operations import calculate_entropic_relevance

import networkx as nx
import warnings
warnings.filterwarnings("ignore")









import pm4py
import json
import os
from pm4py.objects.log.importer.xes import importer as xes_importer
import numpy as np
from math import sqrt

from sklearn.metrics import mean_squared_error, precision_score, recall_score, mean_absolute_error
from scipy.spatial import distance

# from import_data import read_data_equisize, read_data_equitemp, determine_cutoff_point, ActivityPair
# from forecasting import ARf, ARIMAf, SESf, HWf, NAVf, ESf, GARCHf
# from operations import calculate_entropic_relevance

import networkx as nx
import warnings
warnings.filterwarnings("ignore")


############
# Parameters
datasets = ['RTFMP', 'italian', 'Sepsis', 'DomesticDeclarations', 'BPI2019', 'BPI2018', 'BPI2017', 'bpi12', 'InternationalDeclarations']
# datasets = ['italian', 'RTFMP', 'BPI2019', 'InternationalDeclarations', 'DomesticDeclarations']
# dataset = 'InternationalDeclarations'
agg_types = ['equisize', 'equitemp']
no_pairs = 0
horizon = 100
no_intervals = 400
no_folds = 10
no_intervals_all = 500



for dataset in datasets:
    for agg_type in agg_types:
        print('--------------------------new dataset------------------- \n', dataset)
        if not os.path.exists(f'./dataset/{dataset}/'):
            os.makedirs(f'./dataset/{dataset}/')
        # Parameters
        ############


        variant = xes_importer.Variants.ITERPARSE
        paras = {variant.value.Parameters.MAX_TRACES: 1000000000}
        log = xes_importer.apply(dataset + '.xes', parameters=paras)

        # read and encode data
        activity_names = pm4py.get_event_attribute_values(log, 'concept:name')
        no_act = len(activity_names)
        act_map = {}
        reverse_map = {}
        for a, value in enumerate(activity_names.keys()):
            act_map[value] = a
            reverse_map[a] = value

        # add start and end points for DFGs
        act_map['start'] = no_act
        act_map['end'] = no_act + 1
        reverse_map[no_act] = 'start'
        reverse_map[no_act+1] = 'end'
        no_act += 2
        print('Activity encoding:', act_map)

        # store all directly-follows occurrences as activity pairs
        apairs = []
        for t, trace in enumerate(log):
            for e, event in enumerate(trace):
                if e == len(trace) - 1:
                    continue
                ap = ActivityPair(event['concept:name'], trace[e+1]['concept:name'], trace[e+1]['time:timestamp'], event, trace[e+1], t)
                apairs.append(ap)

        sorted_aps = sorted(apairs)
        print('#DFs:', len(sorted_aps))

        interval_width = int(len(sorted_aps) / no_intervals_all)
        print('interval width: ', interval_width)

        # transform the separate DFs into a matrix dfg_time_matrix_org of
        # interval x activities x activities
        if os.path.isfile(f'./matrix/dfg_time_matrix_{dataset}_{no_intervals_all}_{agg_type}_org.npy'):
            with open(f'./matrix/dfg_time_matrix_{dataset}_{no_intervals_all}_{agg_type}_org.npy', 'rb') as f:
                dfg_time_matrix_org = np.load(f)
            print('Read matrix:', dfg_time_matrix_org.shape)
        else:
            print('Creating DFG matrix')
            if agg_type == 'equisize':
                dfg_time_matrix_org, interval_timings = read_data_equisize(no_intervals_all, interval_width, sorted_aps,
                                                                        act_map, log, dataset)
            else:
                dfg_time_matrix_org, interval_timings = read_data_equitemp(no_intervals_all, interval_width, sorted_aps, act_map, log, dataset)

            print('Interval timings: ', interval_timings)
            with open(f'./matrix/dfg_time_matrix_{dataset}_{no_intervals_all}_{agg_type}_org.npy', 'wb') as f:
                np.save(f, dfg_time_matrix_org)

        # determine cutoff in case only the most frequent subset of DF pairs needs to be retained
        if no_pairs == 0:
            cutoff = 0
        else:
            cutoff = determine_cutoff_point(act_map, dfg_time_matrix_org, no_pairs)

        # reduce matrix according to parameter settings
        dfg_time_matrix = dfg_time_matrix_org[:no_intervals, ::, ::]

        techniques = ['nav', 'ar1', 'ar2', 'ar4', 'arima211', 'arima212', 'hw', 'garch', 'ar3', 'ar5', 'ar15', 'arima213', 'arima215', 'arima21n']

        dfg_result_matrix = {}
        dfg_actual_matrix = np.zeros([no_act, no_act, no_folds, horizon], dtype=int)

        for technique in techniques:
            dfg_result_matrix[technique] = np.zeros([no_act, no_act, no_folds, horizon], dtype=float)

        chosen_pairs = set()
        # forecast DFs of all activity pairs
        for act, a in act_map.items():
            for act_2, a2 in act_map.items():

                # by default only time series with at least 1 DF will be selected
                if np.sum(dfg_time_matrix[:, a, a2]) > cutoff:
                    print('DFG', act, 'and', act_2)
                    chosen_pairs.add((a, a2))

                    # get DF
                    array = dfg_time_matrix[:, a, a2]

                    techniques = dict()
                    techniques['nav'] = NAVf()
                    techniques['ar1'] = ARf(1)
                    techniques['ar2'] = ARf(2)
                    techniques['ar4'] = ARf(4)
                    techniques['arima211'] = ARIMAf(2, 1, 1)
                    techniques['arima212'] = ARIMAf(2, 1, 2)
                    techniques['hw'] = HWf()
                    techniques['garch'] = GARCHf()
                    techniques['ar3'] = ARf(3)
                    techniques['ar5'] = ARf(5)
                    techniques['ar15'] = ARf(15)
                    techniques['arima213'] = ARIMAf(2, 1, 3)
                    techniques['arima215'] = ARIMAf(2, 1, 5)
                    techniques['arima21n'] = ARIMAf(2, 1, 15)

                
                    # cross-validation is applied
                    for fold in range(0, no_folds):
                        # offset for cross-validation
                        offset = - fold - horizon
                        x = array[:offset]

                        if fold == 0:
                            y = array[offset:]
                        else:
                            y = array[offset:(offset + horizon)]

                        # store actual
                        dfg_actual_matrix[a, a2, fold] = y

                        for technique, implement in techniques.items():
                            y_pred = []
                            my_x = np.copy(x)

                            try:
                                # predict horizon steps ahead
                                y_hat = implement.fit(my_x, horizon)
                                y_pred = y_hat
                                dfg_result_matrix[technique][a, a2, fold] = y_pred
                            except:
                                dfg_result_matrix[technique][a, a2, fold] = np.full((horizon, ), 100000000)
                    # End results of fold
                # End of -if > cutoff
            # End of act 2
        # End of act 1

        # np.save(f'./dataset/{dataset}/dfg_{no_intervals_all}_{agg_type}_org.npy', dfg_time_matrix_org)
        # np.save(f'./dataset/{dataset}/dfg_{no_intervals_all}_{no_intervals}_{agg_type}.npy', dfg_time_matrix)


        dfg_time_matrix_copy = dfg_time_matrix

        import torch
        import torch.nn as nn

        import numpy as np
        import pandas as pd

        # convert array to Tensor
        # save dataset csv

        def save_interval_dataset(dfg_time_matrix_copy):
            dfg_time_tensor = torch.from_numpy(dfg_time_matrix_copy).float()
            node_cnt = len(dfg_time_matrix_copy[2])
            linear = nn.Linear(node_cnt,1)
            linear_output = linear(dfg_time_tensor)
            dfg_time_input = linear_output.squeeze()
            print('tensor shape:',dfg_time_input.shape)

            dfg_time_np = dfg_time_input.detach().numpy()
            dfg_time_df = pd.DataFrame(dfg_time_np)
            dfg_time_df.to_csv(f'./dataset/{dataset}/{dataset}_input_{no_intervals}_{agg_type}.csv', header = False, index= False)
            dfg_time_df.to_csv(f'./dataset/{dataset}/{dataset}_{no_intervals}_{agg_type}.csv')


        def save_interval_all_dataset(dfg_time_matrix_copy):
            dfg_time_tensor = torch.from_numpy(dfg_time_matrix_copy).float()
            node_cnt = len(dfg_time_matrix_copy[2])
            linear = nn.Linear(node_cnt,1)
            linear_output = linear(dfg_time_tensor)
            dfg_time_input = linear_output.squeeze()
            print('tensor shape:',dfg_time_input.shape)

            dfg_time_np = dfg_time_input.detach().numpy()
            dfg_time_df = pd.DataFrame(dfg_time_np)
            dfg_time_df.to_csv(f'./dataset/{dataset}/{dataset}_input_{no_intervals_all}_{agg_type}.csv', header = False, index= False)
            dfg_time_df.to_csv(f'./dataset/{dataset}/{dataset}_{no_intervals_all}_{agg_type}.csv')


        # save_interval_dataset(dfg_time_matrix_copy)

        # save_interval_all_dataset(dfg_time_matrix_org)



        def MAPE(v, v_, axis=None):
            '''
            Mean absolute percentage error.
            :param v: np.ndarray or int, ground truth.
            :param v_: np.ndarray or int, prediction.
            :param axis: axis to do calculation.
            :return: int, MAPE averages on all elements of input.
            '''
            mape = (np.abs(v_ - v) / (np.abs(v)+1e-5)).astype(np.float64)
            mape = np.where(mape > 5, 5, mape)
            # mape = mape * 100
            return np.mean(mape, axis)


        def RMSE(v, v_, axis=None):
            '''
            Mean squared error.
            :param v: np.ndarray or int, ground truth.
            :param v_: np.ndarray or int, prediction.
            :param axis: axis to do calculation.
            :return: int, RMSE averages on all elements of input.
            '''
            return np.sqrt(np.mean((v_ - v) ** 2, axis)).astype(np.float64)


        def MAE(v, v_, axis=None):
            '''
            Mean absolute error.
            :param v: np.ndarray or int, ground truth.
            :param v_: np.ndarray or int, prediction.
            :param axis: axis to do calculation.
            :return: int, MAE averages on all elements of input.
            '''
            return np.mean(np.abs(v_ - v), axis).astype(np.float64)

        # Results per horizon/fold
        with open(f'result_correct/results_{dataset}_nopairs_{no_pairs}_nointervals_{str(no_intervals)}_{agg_type}.csv', 'w') as technique_fold_results:
            technique_fold_results.write('intervals,technique,fold,horizon,rmse,mape,mae,er_pred,er_actual\n')

            for technique in techniques:
                print(f'Technique {technique}')
                dfg_result_matrix_ar = dfg_result_matrix[technique]
                results_selected = np.zeros((len(chosen_pairs), no_folds, horizon))
                actual_selected = np.zeros((len(chosen_pairs), no_folds, horizon))

                for p, pair in enumerate(chosen_pairs):
                    results_selected[p] = dfg_result_matrix_ar[pair[0], pair[1], ::, ::]
                    actual_selected[p] = dfg_actual_matrix[pair[0], pair[1], ::, ::]

                for h in range(0, horizon):
                    for fold in range(0, no_folds):
                        # print(f'Fold {fold} - horizon {h}')

                        #####################
                        # Forecast DFG output
                        nodes = []
                        for act_name, act_code in act_map.items():
                            out_freq = int(np.sum(dfg_result_matrix_ar[act_code, ::, fold, h]))
                            in_freq = int(np.sum(dfg_result_matrix_ar[::, act_code, fold, h]))

                            if act_name == 'start':
                                nodes.append({'label': str(act_code), 'freq': out_freq, 'id': act_code})
                            else:
                                if in_freq > 0:
                                    nodes.append({'label': str(act_code), 'freq': in_freq, 'id': act_code})

                        arcs = []
                        for a in range(0, len(act_map)):
                            for a2 in range(0, len(act_map)):
                                if int(dfg_result_matrix_ar[a, a2, fold, h]) > 0:
                                    arcs.append({'from': a, 'to': a2, 'freq': int(dfg_result_matrix_ar[a, a2, fold, h])})

                        # calculate entropic relevance
                        dfg_file = {'nodes': nodes, 'arcs': arcs}
                        r = json.dumps(dfg_file, indent=1)
                        with open(f'temp.json', 'w') as dfg_write_file:
                            dfg_write_file.write(r)

                        offset = no_intervals - horizon + h - fold
                        # er_technique = calculate_entropic_relevance(f'./logs/{dataset}_log_interval_{offset}-{no_intervals_all}_{agg_type}', 'temp')


                        ###################
                        # Actual DFG output
                        nodes = []
                        for act_name, act_code in act_map.items():
                            out_freq = int(np.sum(dfg_actual_matrix[act_code, ::, fold, h]))
                            in_freq = int(np.sum(dfg_actual_matrix[::, act_code, fold, h]))

                            if act_name == 'start':
                                nodes.append({'label': str(act_code), 'freq': out_freq, 'id': act_code})
                            else:
                                if in_freq > 0:
                                    nodes.append({'label': str(act_code), 'freq': in_freq, 'id': act_code})

                        arcs = []
                        for a in range(0, len(act_map)):
                            for a2 in range(0, len(act_map)):
                                if int(dfg_actual_matrix[a, a2, fold, h]) > 0:
                                    arcs.append({'from': a, 'to': a2, 'freq': int(dfg_actual_matrix[a, a2, fold, h])})

                        # calculate entropic relevance
                        dfg_file = {'nodes': nodes, 'arcs': arcs}
                        r = json.dumps(dfg_file, indent=1)
                        with open(f'temp.json', 'w') as dfg_write_file:
                            dfg_write_file.write(r)

                        offset = no_intervals - horizon + h - fold
                        # er_actual = calculate_entropic_relevance(f'./logs/{dataset}_log_interval_{offset}-{no_intervals_all}_{agg_type}', 'temp')

                        # store results
                        results = np.reshape(results_selected[::, fold, h], (1, len(chosen_pairs)))
                        actuals = np.reshape(actual_selected[::, fold, h], (1, len(chosen_pairs)))

                        # cosine = distance.cosine(results, actuals)
                        # rmse = sqrt(mean_squared_error(actuals, results))
                        rmse = RMSE(actuals, results)
                        mape = MAPE(actuals, results)
                        mae = MAE(actuals, results)

                        # technique_fold_results.write(f'{no_intervals},{technique},{str(fold)},{str(h)},{cosine},{rmse},{er_technique},{er_actual}\n')
                        technique_fold_results.write(f'{no_intervals},{technique},{str(fold)},{str(h)},{rmse},{mape},{mae}\n')

        def evaluate(tech):
        
            dfg_result_matrix_ar = dfg_result_matrix[tech]
            #   print('dfg_actual_matrix: ', dfg_actual_matrix)
            #   print('dfg_result_matrix_ar: ', dfg_result_matrix_ar)
            rmse = RMSE(dfg_actual_matrix, dfg_result_matrix_ar)
            mape = MAPE(dfg_actual_matrix, dfg_result_matrix_ar)
            mae = MAE(dfg_actual_matrix, dfg_result_matrix_ar)

            return rmse, mape, mae




        # 'nav', 'ar1', 'ar2', 'ar4', 'arima211', 'arima212', 'hw', 'garch'
        nav = evaluate("nav")
        print('nav: ', nav)

        ar1 = evaluate("ar1")
        print('ar1: ', ar1)

        ar2 = evaluate("ar2")
        print('ar2: ', ar2)

        ar4 = evaluate("ar4")
        print('ar4: ', ar4)

        arima211 = evaluate("arima211")
        print('arima211: ',arima211)

        arima212 = evaluate("arima212")
        print('arima212: ', arima212)

        hw= evaluate("hw")
        print('hw: ', hw)

        gar = evaluate("garch")
        print('gar: ', gar)

        print(ar1)
        print(arima211)


        ar3 = evaluate("ar3")
        print('ar3: ', ar3)
        ar5 = evaluate("ar5")
        print('ar5: ', ar5)
        ar15 = evaluate("ar15")
        print('ar15: ', ar15)


        arima213 = evaluate("arima213")
        print('arima213: ', arima213)
        arima215 = evaluate("arima215")
        print('arima215: ', arima215)
        arima21n = evaluate("arima21n")
        print('arima21n: ', arima21n)
